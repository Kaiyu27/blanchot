{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "18d38d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CREATE BlanchotWork OBJECT TO STANDARDIZE DATA STRUCTURE OF REPSECTIVE DATABASES (OA, CR, HAL, PP)\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e79881fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(BaseModel):\n",
    "    \"\"\"A standardized representation of an author.\"\"\"\n",
    "    full_name: str\n",
    "    given_name: Optional[str] = None\n",
    "    family_name: Optional[str] = None\n",
    "\n",
    "class BlanchotWork(BaseModel):\n",
    "    \"\"\"\n",
    "    A comprehensive, standardized model for a single bibliographic record,\n",
    "    designed to hold rich data from multiple sources.\n",
    "    \"\"\"\n",
    "    # Core Identifiers\n",
    "    doi: Optional[str] = None\n",
    "    title: str\n",
    "    \n",
    "    # Author Details\n",
    "    authors: List[Author]\n",
    "    \n",
    "    # Publication Details\n",
    "    year: Optional[int] = None\n",
    "    publication_date: Optional[str] = None\n",
    "    journal_name: Optional[str] = None\n",
    "    publisher: Optional[str] = None\n",
    "    work_type: Optional[str] = None\n",
    "    \n",
    "    # Content and Context\n",
    "    abstract: Optional[str] = None\n",
    "    subjects: List[str] = []\n",
    "    \n",
    "    # Metrics and Links\n",
    "    citation_count: Optional[int] = None\n",
    "    source_url: HttpUrl # A direct link to the record in its original database\n",
    "    \n",
    "    # Metadata\n",
    "    source_db: str # To track where it came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c785862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_openalex_to_blanchotwork(work_data: dict) -> dict:\n",
    "    \"\"\"Translates a raw OpenAlex dictionary into our standard format.\"\"\"\n",
    "    authors = []\n",
    "    for authorship in work_data.get('authorships', []):\n",
    "        if author_info := authorship.get('author'):\n",
    "            authors.append(Author(full_name=author_info.get('display_name', '')))\n",
    "\n",
    "    subjects = [concept.get('display_name') for concept in work_data.get('concepts', []) if concept]\n",
    "    journal = work_data.get('primary_location', {}).get('source', {}).get('display_name')\n",
    "\n",
    "    return {\n",
    "        'doi': work_data.get('doi'),\n",
    "        'title': work_data.get('title', 'No Title Provided'),\n",
    "        'authors': authors,\n",
    "        'year': work_data.get('publication_year'),\n",
    "        'publication_date': work_data.get('publication_date'),\n",
    "        'journal_name': journal,\n",
    "        'publisher': work_data.get('publisher'),\n",
    "        'work_type': work_data.get('type'),\n",
    "        'subjects': subjects,\n",
    "        'source_url': work_data.get('id'),\n",
    "        'source_db': 'OpenAlex'\n",
    "    }\n",
    "\n",
    "def from_crossref_to_blanchotwork(work_data: dict) -> dict:\n",
    "    \"\"\"Translates a raw Crossref dictionary into our standard format.\"\"\"\n",
    "    authors = []\n",
    "    author_list = work_data.get('author') or [] \n",
    "    for author_info in author_list:\n",
    "        given = author_info.get('given', '')\n",
    "        family = author_info.get('family', '')\n",
    "        authors.append(Author(full_name=f\"{given} {family}\".strip(), given_name=given, family_name=family))\n",
    "\n",
    "    year = None\n",
    "    if published := (work_data.get('published-print') or work_data.get('published-online')):\n",
    "        if date_parts := published.get('date-parts', [[]]):\n",
    "            year = date_parts[0][0] if date_parts[0] else None\n",
    "\n",
    "    # --- ✅ FIX IS HERE ---\n",
    "    # Safely extract the journal name by checking its type first\n",
    "    journal_name = None\n",
    "    container_title = work_data.get('container-title')\n",
    "    if isinstance(container_title, list) and container_title:\n",
    "        journal_name = container_title[0]\n",
    "    # ----------------------\n",
    "\n",
    "    doi = work_data.get('DOI')\n",
    "    source_url = work_data.get('URL')\n",
    "    if not source_url and doi:\n",
    "        source_url = f\"https://doi.org/{doi}\"\n",
    "    \n",
    "    return {\n",
    "        'doi': doi,\n",
    "        'title': (work_data.get('title') or ['No Title Provided'])[0],\n",
    "        'authors': authors,\n",
    "        'year': year,\n",
    "        'journal_name': journal_name, # Use the safely extracted name\n",
    "        'publisher': work_data.get('publisher'),\n",
    "        'work_type': work_data.get('type'),\n",
    "        'subjects': work_data.get('subject', []),\n",
    "        'source_url': source_url,\n",
    "        'source_db': 'Crossref'\n",
    "    }\n",
    "\n",
    "def from_hal_to_blanchotwork(work_data: dict) -> dict:\n",
    "    \"\"\"Translates a raw HAL dictionary into our standard format.\"\"\"\n",
    "    authors = [Author(full_name=name) for name in work_data.get('authFullName_s', [])]\n",
    "\n",
    "    return {\n",
    "        'doi': work_data.get('doiId_s'), # HAL often stores DOI in this field\n",
    "        'title': (work_data.get('title_s') or ['No Title Provided'])[0],\n",
    "        'authors': authors,\n",
    "        'year': work_data.get('publicationDateY_i'),\n",
    "        'journal_name': work_data.get('journalTitle_s'),\n",
    "        'work_type': work_data.get('docType_s'),\n",
    "        'source_url': work_data.get('uri_s'),\n",
    "        'source_db': 'HAL'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "284c64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openalex/openalex_blanchot.json', mode='rt') as f:\n",
    "    openalex_json = json.load(f)\n",
    "    \n",
    "with open('crossref/crossref_blanchot_filtered.json', mode='rt') as f:\n",
    "    crossref_json = json.load(f)\n",
    "\n",
    "with open('hal/hal_blanchot_data.json', mode='rt') as f:\n",
    "    hal_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "90cf0557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthesis complete. 582 unique works.\n"
     ]
    }
   ],
   "source": [
    "# TODO: COMPLETE TRANSLATOR FROM RESPECTIVE DATABASES TO BlanchotWork OBJECTS.\n",
    "\n",
    "master_list: List[BlanchotWork] = []\n",
    "failed_records = []\n",
    "\n",
    "# --- Process OpenAlex Data ---\n",
    "for work in openalex_json:\n",
    "    try:\n",
    "        # ✅ FIX: Check if the item is a string and parse it if necessary\n",
    "        if isinstance(work, str):\n",
    "            work_dict = json.loads(work)\n",
    "        else:\n",
    "            work_dict = work\n",
    "\n",
    "        standardized_data = from_openalex_to_blanchotwork(work_dict)\n",
    "        master_list.append(BlanchotWork.model_validate(standardized_data))\n",
    "        \n",
    "    except (ValidationError, json.JSONDecodeError) as e:\n",
    "        # This now catches both Pydantic and JSON parsing errors\n",
    "        failed_records.append({'source': 'OpenAlex', 'id': work.get('id') if isinstance(work, dict) else 'Unknown', 'error': str(e)})\n",
    "\n",
    "# --- Process Crossref Data ---\n",
    "for work in crossref_json:\n",
    "    standardized_data = from_crossref_to_blanchotwork(work)\n",
    "    try:\n",
    "        master_list.append(BlanchotWork.model_validate(standardized_data))\n",
    "    except ValidationError as e:\n",
    "        failed_records.append({'source': 'Crossref', 'id': work.get('DOI'), 'error': str(e)})\n",
    "        \n",
    "# --- Process HAL Data ---\n",
    "for work in hal_json:\n",
    "    standardized_data = from_hal_to_blanchotwork(work)\n",
    "    try:\n",
    "        master_list.append(BlanchotWork.model_validate(standardized_data))\n",
    "    except ValidationError as e:\n",
    "        failed_records.append({'source': 'HAL', 'id': work.get('uri_s'), 'error': str(e)})\n",
    "\n",
    "# --- Final Step: De-duplicate the master list ---\n",
    "unique_works = {}\n",
    "for work in master_list:\n",
    "    if work.doi: # Use DOI as the primary key for de-duplication\n",
    "        unique_works[work.doi.lower()] = work\n",
    "\n",
    "final_master_list = list(unique_works.values())\n",
    "\n",
    "print(f\"✅ Synthesis complete. {len(final_master_list)} unique works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "050a21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_to_save = [work.model_dump(mode='json') for work in final_master_list]\n",
    "with open('blanchot_master_list.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(records_to_save, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ce59250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simplified list for review...\n",
      "Created simplified list with 582 records.\n",
      "\n",
      "Saving data to JSON file: blanchot_master_list_quick_review.json...\n",
      "✅ Successfully saved JSON file.\n",
      "\n",
      "Flattening data for CSV export...\n",
      "✅ Successfully saved flattened data to CSV file: blanchot_master_list_quick_review.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'final_master_list' is your list of validated Pydantic 'BlanchotWork' objects.\n",
    "\n",
    "# --- 1. Create the simplified list for quick review ---\n",
    "# This step is done only once.\n",
    "print(\"Creating simplified list for review...\")\n",
    "quick_review_list = [\n",
    "    {\n",
    "        \"year\": work.year,\n",
    "        \"title\": work.title,\n",
    "        \"authors\": [author.full_name for author in work.authors],\n",
    "        \"source_url\": str(work.source_url)\n",
    "    }\n",
    "    for work in final_master_list\n",
    "]\n",
    "print(f\"Created simplified list with {len(quick_review_list)} records.\")\n",
    "\n",
    "\n",
    "# --- 2. Save the list to a JSON file ---\n",
    "# This preserves the list of authors within the JSON structure.\n",
    "json_filename = 'blanchot_master_list_quick_review.json'\n",
    "print(f\"\\nSaving data to JSON file: {json_filename}...\")\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(quick_review_list, f, indent=2)\n",
    "print(f\"✅ Successfully saved JSON file.\")\n",
    "\n",
    "\n",
    "# --- 3. Flatten the list and save it to a CSV file ---\n",
    "# This converts the list of authors into a single string for the CSV.\n",
    "print(f\"\\nFlattening data for CSV export...\")\n",
    "records_for_csv = []\n",
    "for record in quick_review_list:\n",
    "    records_for_csv.append({\n",
    "        'year': record['year'],\n",
    "        'title': record['title'],\n",
    "        'authors': ', '.join(record['authors']), # Join the list into a single string\n",
    "        'source_url': record['source_url']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records_for_csv)\n",
    "csv_filename = 'blanchot_master_list_quick_review.csv'\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "print(f\"✅ Successfully saved flattened data to CSV file: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81b69a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simplified list for review...\n",
      "Created simplified list with 582 records.\n",
      "\n",
      "Saving data to JSON file: blanchot_master_list_quick_review.json...\n",
      "✅ Successfully saved JSON file.\n",
      "\n",
      "Consolidating book chapters for CSV export...\n",
      "✅ Successfully saved consolidated data to CSV file: blanchot_master_list_consolidated.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Creating simplified list for review...\")\n",
    "quick_review_list = [\n",
    "    {\n",
    "        \"year\": work.year,\n",
    "        \"title\": work.title,\n",
    "        \"authors\": [author.full_name for author in work.authors],\n",
    "        \"source_url\": str(work.source_url)\n",
    "    }\n",
    "    for work in final_master_list\n",
    "]\n",
    "print(f\"Created simplified list with {len(quick_review_list)} records.\")\n",
    "\n",
    "json_filename = 'blanchot_master_list_quick_review.json'\n",
    "print(f\"\\nSaving data to JSON file: {json_filename}...\")\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(quick_review_list, f, indent=2)\n",
    "print(f\"✅ Successfully saved JSON file.\")\n",
    "\n",
    "print(f\"\\nConsolidating book chapters for CSV export...\")\n",
    "\n",
    "df = pd.DataFrame(quick_review_list)\n",
    "\n",
    "df['book_title'] = df['title'].str.split(':').str[0].str.strip()\n",
    "\n",
    "consolidated_df = df.groupby(['book_title', 'year']).agg(\n",
    "    authors=('authors', 'first'),\n",
    "    source_url=('source_url', 'first'),\n",
    "    chapters_found=('title', 'count')\n",
    ").reset_index()\n",
    "\n",
    "def create_final_title(row):\n",
    "    if row['chapters_found'] > 1:\n",
    "        return f\"{row['book_title']} ({row['chapters_found']} chapters)\"\n",
    "    else:\n",
    "        return row['book_title']\n",
    "\n",
    "consolidated_df['title'] = consolidated_df.apply(create_final_title, axis=1)\n",
    "\n",
    "consolidated_df['authors'] = consolidated_df['authors'].apply(lambda authors: ', '.join(authors))\n",
    "\n",
    "final_df = consolidated_df[['year', 'title', 'authors', 'source_url', 'chapters_found']]\n",
    "final_df = final_df.sort_values(by='year', ascending=False)\n",
    "\n",
    "csv_filename = 'blanchot_master_list_consolidated.csv'\n",
    "final_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "print(f\"✅ Successfully saved consolidated data to CSV file: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7bf57018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 📊 DIAGNOSTICS: Initial Record Count ---\n",
      "Loaded 4067 records from OpenAlex.\n",
      "Loaded 582 records from Crossref.\n",
      "Loaded 180 records from HAL.\n",
      "Total raw records: 4829\n",
      "--------------------\n",
      "--- 📊 DIAGNOSTICS: After Validation ---\n",
      "Total records that passed validation: 237\n",
      "--------------------\n",
      "Consolidating book chapters from 237 total records...\n",
      "--- 📊 DIAGNOSTICS: After Chapter Consolidation ---\n",
      "Consolidation resulted in 230 unique works/books.\n",
      "--------------------\n",
      "--- 📊 DIAGNOSTICS: After Final De-duplication by DOI ---\n",
      "Final master list contains 51 unique works.\n",
      "--------------------\n",
      "\n",
      "✅ Synthesis complete. Saved a master list with 51 unique, consolidated works.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError\n",
    "\n",
    "# --- (Your Pydantic models and translator functions go here, unchanged) ---\n",
    "\n",
    "# --- MAIN SCRIPT WORKFLOW with DIAGNOSTICS ---\n",
    "\n",
    "# Load source files\n",
    "with open('openalex/openalex_blanchot.json', mode='rt') as f: openalex_json = json.load(f)\n",
    "with open('crossref/crossref_blanchot_filtered.json', mode='rt') as f: crossref_json = json.load(f)\n",
    "with open('hal/hal_blanchot_data.json', mode='rt') as f: hal_json = json.load(f)\n",
    "\n",
    "print(\"--- 📊 DIAGNOSTICS: Initial Record Count ---\")\n",
    "print(f\"Loaded {len(openalex_json)} records from OpenAlex.\")\n",
    "print(f\"Loaded {len(crossref_json)} records from Crossref.\")\n",
    "print(f\"Loaded {len(hal_json)} records from HAL.\")\n",
    "print(f\"Total raw records: {len(openalex_json) + len(crossref_json) + len(hal_json)}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "master_list: List[BlanchotWork] = []\n",
    "failed_records = []\n",
    "\n",
    "# (Processing loops for OpenAlex, Crossref, and HAL are unchanged)\n",
    "for source_data, translator, source_name in [(openalex_json, from_openalex_to_blanchotwork, \"OpenAlex\"), (crossref_json, from_crossref_to_blanchotwork, \"Crossref\"), (hal_json, from_hal_to_blanchotwork, \"HAL\")]:\n",
    "    for work in source_data:\n",
    "        try:\n",
    "            work_dict = json.loads(work) if isinstance(work, str) else work\n",
    "            standardized_data = translator(work_dict)\n",
    "            master_list.append(BlanchotWork.model_validate(standardized_data))\n",
    "        except (ValidationError, json.JSONDecodeError):\n",
    "            continue\n",
    "\n",
    "print(f\"--- 📊 DIAGNOSTICS: After Validation ---\")\n",
    "print(f\"Total records that passed validation: {len(master_list)}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- CONSOLIDATE CHAPTERS ---\n",
    "print(f\"Consolidating book chapters from {len(master_list)} total records...\")\n",
    "df = pd.DataFrame([work.model_dump() for work in master_list])\n",
    "\n",
    "def get_grouping_id(row):\n",
    "    if pd.notna(row.get('relation')) and 'is-part-of' in row['relation']:\n",
    "        if parent_doi := row['relation']['is-part-of'][0].get('id'):\n",
    "            return f\"DOI:{parent_doi}\"\n",
    "    title_prefix = str(row['title']).split(':')[0].strip()\n",
    "    return f\"TITLE:{title_prefix}_{row['year']}\"\n",
    "\n",
    "df['grouping_id'] = df.apply(get_grouping_id, axis=1)\n",
    "\n",
    "consolidated_df = df.groupby('grouping_id').agg(\n",
    "    doi=('doi', 'first'), year=('year', 'first'), authors=('authors', 'first'),\n",
    "    publisher=('publisher', 'first'), work_type=('work_type', 'first'),\n",
    "    source_url=('source_url', 'first'), source_db=('source_db', 'first'),\n",
    "    title=('title', lambda x: min(x, key=len)),\n",
    "    chapters_found=('title', 'count')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"--- 📊 DIAGNOSTICS: After Chapter Consolidation ---\")\n",
    "print(f\"Consolidation resulted in {len(consolidated_df)} unique works/books.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- FINAL DE-DUPLICATION and SAVE ---\n",
    "\n",
    "# This line is correct\n",
    "final_df = consolidated_df.dropna(subset=['doi']).drop_duplicates(subset=['doi'], keep='first')\n",
    "\n",
    "print(f\"--- 📊 DIAGNOSTICS: After Final De-duplication by DOI ---\")\n",
    "print(f\"Final master list contains {len(final_df)} unique works.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- ✅ FIX: Convert columns with special objects to strings before saving ---\n",
    "# This ensures all data is in a format that the json library can handle.\n",
    "final_df['source_url'] = final_df['source_url'].astype(str)\n",
    "# It's also good practice to convert the 'authors' column, as it contains Author objects\n",
    "final_df['authors'] = final_df['authors'].astype(str)\n",
    "\n",
    "\n",
    "# Now, convert the clean DataFrame to a list of dictionaries\n",
    "records_to_save = final_df.to_dict('records')\n",
    "\n",
    "# Save the final list to your JSON file\n",
    "with open('blanchot_master_list.json', 'w', encoding='utf-8') as f:\n",
    "    # This will now work because all special objects have been converted to strings\n",
    "    json.dump(records_to_save, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Synthesis complete. Saved a master list with {len(records_to_save)} unique, consolidated works.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blanchot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
