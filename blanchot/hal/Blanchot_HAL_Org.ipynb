{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aba3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afef89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HALWorkModel(BaseModel):\n",
    "    title_s: List[str]\n",
    "    docType_s: str\n",
    "    uri_s: HttpUrl\n",
    "    authFullName_s: Optional[List[str]] = None\n",
    "    publicationDateY_i: Optional[int] = None\n",
    "    journalTitle_s: Optional[str] = None\n",
    "    class Config:\n",
    "        extra = 'allow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bcc8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying HAL API to get total number of results...\n",
      "Found 180 total works to download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading works from HAL: 100%|██████████| 180/180 [00:01<00:00, 120.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------\n",
      "Download complete. Total validated records: 180\n",
      "\n",
      "No duplicate entries were found.\n",
      "\n",
      "Saving all 180 validated records (including duplicates)...\n",
      "✅ Successfully saved all records to 'hal_blanchot_data.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://api.archives-ouvertes.fr/search/\"\n",
    "SEARCH_TERM = \"Blanchot\"\n",
    "START_YEAR = 1998\n",
    "ROWS_PER_PAGE = 100\n",
    "OUTPUT_JSON_FILE = \"hal_blanchot_data.json\"\n",
    "\n",
    "validated_works = []\n",
    "failed_works_log = []\n",
    "start = 0\n",
    "num_found = 0\n",
    "\n",
    "print(\"Querying HAL API to get total number of results...\")\n",
    "specific_query = f'(title_t:\"{SEARCH_TERM}\" OR abstract_t:\"{SEARCH_TERM}\")'\n",
    "initial_params = {'q': specific_query, 'fq': f'publicationDateY_i:[{START_YEAR} TO *]', 'rows': 0}\n",
    "try:\n",
    "    initial_resp = requests.get(BASE_URL, params=initial_params).json()\n",
    "    num_found = initial_resp.get('response', {}).get('numFound', 0)\n",
    "    print(f\"Found {num_found} total works to download.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Initial API request failed: {e}\")\n",
    "    num_found = 0\n",
    "\n",
    "if num_found > 0:\n",
    "    with tqdm(total=num_found, desc=\"Downloading works from HAL\") as pbar:\n",
    "        while start < num_found:\n",
    "            params = {\n",
    "                'q': f'(title_t:\"{SEARCH_TERM}\" OR abstract_t:\"{SEARCH_TERM}\")',\n",
    "                'fq': f'publicationDateY_i:[{START_YEAR} TO *]',\n",
    "                'fl': 'title_s, authFullName_s, publicationDateY_i, journalTitle_s, uri_s, docType_s, docid',\n",
    "                'wt': 'json',\n",
    "                'rows': ROWS_PER_PAGE,\n",
    "                'start': start,\n",
    "                'sort': 'docid asc'\n",
    "            }\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                docs = data.get('response', {}).get('docs', [])\n",
    "                if not docs: break\n",
    "                for doc_data in docs:\n",
    "                    try:\n",
    "                        validated_work = HALWorkModel.model_validate(doc_data)\n",
    "                        validated_works.append(validated_work)\n",
    "                    except ValidationError as e:\n",
    "                        failed_works_log.append({\"uri\": doc_data.get(\"uri_s\"), \"error_details\": e.errors()})\n",
    "                pbar.update(len(docs))\n",
    "                start += ROWS_PER_PAGE\n",
    "                time.sleep(0.1)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nAn error occurred during download: {e}\"); break\n",
    "\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(f\"Download complete. Total validated records: {len(validated_works)}\")\n",
    "\n",
    "records_by_id = defaultdict(list)\n",
    "for work in validated_works:\n",
    "    work_dict = work.model_dump() \n",
    "    work_id = work_dict.get('uri_s')\n",
    "    if work_id:\n",
    "        records_by_id[work_id].append(work_dict)\n",
    "\n",
    "duplicate_ids = []\n",
    "for work_id, works_list in records_by_id.items():\n",
    "    if len(works_list) > 1:\n",
    "        duplicate_ids.append(work_id)\n",
    "\n",
    "if duplicate_ids:\n",
    "    print(f\"\\nFound {len(duplicate_ids)} IDs with duplicate entries:\")\n",
    "    print('\\n'.join([str(url) for url in duplicate_ids]))\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo duplicate entries were found.\")\n",
    "\n",
    "\n",
    "print(f\"\\nSaving all {len(validated_works)} validated records (including duplicates)...\")\n",
    "\n",
    "records_to_save = [work.model_dump(mode='json') for work in validated_works]\n",
    "\n",
    "with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(records_to_save, f, indent=2)\n",
    "\n",
    "print(f\"✅ Successfully saved all records to '{OUTPUT_JSON_FILE}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blanchot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
