{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37295ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/blanchot/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO:RUN SCRIPT WEEKLY W/ GITHUB BOT\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, HttpUrl, Field, ValidationError\n",
    "from crossref.restful import Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ae4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateParts(BaseModel):\n",
    "    date_parts: Optional[List[List[Optional[int]]]] = Field(None, alias='date-parts')\n",
    "\n",
    "class Author(BaseModel):\n",
    "    given: Optional[str] = None\n",
    "    family: Optional[str] = None\n",
    "    sequence: Optional[str] = None\n",
    "    affiliation: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class CrossrefWorkModel(BaseModel):\n",
    "    DOI: str\n",
    "    title: List[str]\n",
    "    author: Optional[List[Author]] = None\n",
    "    publisher: str\n",
    "    type: str\n",
    "    published_print: Optional[DateParts] = Field(None, alias='published-print')\n",
    "    \n",
    "    class Config:\n",
    "        extra = 'allow' \n",
    "        populate_by_name = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8d446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2405/2405 [02:23<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.\n",
      "Total validated records: 2405\n",
      "Number of unique records after de-duplication: 2405\n",
      "Number of duplicate records found and removed: 0\n",
      "\n",
      "Created DataFrame with 2405 records for filtering.\n",
      "\n",
      "Found 1145 records from publishers matching academic keywords.\n",
      "\n",
      " Sample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1103/physrevlett.80.1658</td>\n",
       "      <td>[Dynamics of Subpicosecond Relativistic Laser ...</td>\n",
       "      <td>American Physical Society (APS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.3828/ajfs.35.2.228</td>\n",
       "      <td>[Transgression, Masochism and Subjectivity: th...</td>\n",
       "      <td>Liverpool University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.17161/chimeres.v25i1.6161</td>\n",
       "      <td>[Maurice Blanchot: Littérature et ruine de l'é...</td>\n",
       "      <td>The University of Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.1093/fs/lii.4.488</td>\n",
       "      <td>[REVIEWS Maurice Blanchot et le déplacement d'...</td>\n",
       "      <td>Liverpool University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.1103/physrevlett.81.4275</td>\n",
       "      <td>[Fuchs&lt;i&gt;et al.&lt;/i&gt;Reply:]</td>\n",
       "      <td>American Physical Society (APS)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             DOI  \\\n",
       "18   10.1103/physrevlett.80.1658   \n",
       "21         10.3828/ajfs.35.2.228   \n",
       "24  10.17161/chimeres.v25i1.6161   \n",
       "25          10.1093/fs/lii.4.488   \n",
       "26   10.1103/physrevlett.81.4275   \n",
       "\n",
       "                                                title  \\\n",
       "18  [Dynamics of Subpicosecond Relativistic Laser ...   \n",
       "21  [Transgression, Masochism and Subjectivity: th...   \n",
       "24  [Maurice Blanchot: Littérature et ruine de l'é...   \n",
       "25  [REVIEWS Maurice Blanchot et le déplacement d'...   \n",
       "26                         [Fuchs<i>et al.</i>Reply:]   \n",
       "\n",
       "                          publisher  \n",
       "18  American Physical Society (APS)  \n",
       "21       Liverpool University Press  \n",
       "24         The University of Kansas  \n",
       "25       Liverpool University Press  \n",
       "26  American Physical Society (APS)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "works_api = Works()\n",
    "\n",
    "works_query = works_api.query(bibliographic='Blanchot').filter(\n",
    "    from_pub_date='1998'\n",
    ").sort('published').order('asc')\n",
    "\n",
    "validated_records = []\n",
    "failed_records = []\n",
    "\n",
    "try:\n",
    "    for work_data in tqdm(works_query, total=works_query.count(), desc=\"Downloading\"):\n",
    "        try:\n",
    "            validated_work = CrossrefWorkModel.model_validate(work_data)\n",
    "            validated_records.append(validated_work)\n",
    "        except ValidationError as e:\n",
    "            failed_records.append({'doi': work_data.get('DOI'), 'error': str(e)})\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during download: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nDownload complete.\")\n",
    "print(f\"Total validated records: {len(validated_records)}\")\n",
    "if failed_records:\n",
    "    print(f\"Total records that failed validation: {len(failed_records)}\")\n",
    "\n",
    "\n",
    "unique_records = []\n",
    "seen_dois = set()\n",
    "duplicate_log = []\n",
    "\n",
    "for work in validated_records:\n",
    "    doi = work.DOI\n",
    "\n",
    "    if doi not in seen_dois:\n",
    "        unique_records.append(work)\n",
    "        seen_dois.add(doi)\n",
    "    else:\n",
    "        duplicate_log.append({\n",
    "            \"DOI\": doi,\n",
    "            \"title\": work.title[0] if work.title else \"No Title\"\n",
    "        })\n",
    "\n",
    "\n",
    "print(f\"Number of unique records after de-duplication: {len(unique_records)}\")\n",
    "print(f\"Number of duplicate records found and removed: {len(duplicate_log)}\")\n",
    "\n",
    "# Post-Download Filtering by Publisher/Journal Name\n",
    "\n",
    "if unique_records:\n",
    "    df = pd.DataFrame([work.model_dump(by_alias=True) for work in unique_records])\n",
    "    print(f\"\\nCreated DataFrame with {len(df)} records for filtering.\")\n",
    "\n",
    "    if 'publisher' in df.columns:\n",
    "        # TODO:EXPAND LIST -- EXPANDED BELOW\n",
    "        academic_keywords = [\n",
    "            # --- Core Disciplines & Theories ---\n",
    "            'Philosophy', 'Philosophie', 'Filosofia', 'Filosofía',\n",
    "            'Literature', 'Literary', 'Linguistics', 'Poetics',\n",
    "            'Humanities', 'Theory', 'Critical', 'Deconstruction',\n",
    "            'Phenomenology', 'Psychoanalysis', 'Aesthetics', 'Cultural Studies',\n",
    "            \n",
    "            # --- Institutional & Publisher Types ---\n",
    "            'University Press', 'University', 'Press', 'Academic',\n",
    "            'College', 'Institute', 'Institut', 'Centro', 'Centre',\n",
    "            'Society', 'Société', 'Sociedad',\n",
    "            \n",
    "            # --- Publication Types (English) ---\n",
    "            'Journal', 'Review', 'Studies', 'Quarterly', 'Annual', 'Annals',\n",
    "            'Proceedings', 'Transactions', 'Bulletin', 'Archive', 'Yearbook',\n",
    "            \n",
    "            # --- Publication Types (Foreign Languages) ---\n",
    "            # French\n",
    "            'Revue', 'Cahiers', 'Études', 'Annales', 'Presses',\n",
    "            # German\n",
    "            'Zeitschrift', 'Kritik', 'Jahrbuch', 'Archiv', 'Verlag',\n",
    "            # Italian\n",
    "            'Rivista', 'Studi', 'Annali',\n",
    "            # Spanish / Portuguese\n",
    "            'Revista', 'Estudios', 'Anales',\n",
    "            # Latin\n",
    "            'Acta'\n",
    "        ]\n",
    "        \n",
    "        search_pattern = '|'.join(academic_keywords)\n",
    "        \n",
    "        df_filtered = df[df['publisher'].str.contains(search_pattern, case=False, na=False)]\n",
    "        \n",
    "        print(f\"\\nFound {len(df_filtered)} records from publishers matching academic keywords.\")\n",
    "        \n",
    "        print(\"\\n Sample\")\n",
    "        display(df_filtered[['DOI', 'title', 'publisher']].head())\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"\\nWarning: 'publisher' column not found in the downloaded data. Cannot perform filtering.\")\n",
    "        df.to_csv('crossref_blanchot_unfiltered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf2850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1145 filtered records to JSON file...\n",
      "File 'crossref_blanchot_filtered.json' created successfully.\n"
     ]
    }
   ],
   "source": [
    "#FILE CREATION\n",
    "records_saved = df_filtered.to_dict('records')\n",
    "\n",
    "print(f\"Saving {len(records_saved)} filtered records to JSON file...\")\n",
    "with open('crossref_blanchot_filtered.json', mode='w', encoding='utf-8') as f:\n",
    "    json.dump(records_saved, f, indent=4)\n",
    "    \n",
    "print(\"File 'crossref_blanchot_filtered.json' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cac3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import ValidationError\n",
    "from crossref.restful import Works\n",
    "from models import CrossrefWorkModel\n",
    "\n",
    "works_api = Works()\n",
    "\n",
    "works_query = works_api.query(bibliographic='Blanchot').filter(\n",
    "    from_pub_date='1998'\n",
    ").sort('published').order('asc')\n",
    "\n",
    "def get_cr_work():\n",
    "    validated_records = []\n",
    "    failed_records = []\n",
    "\n",
    "    try:\n",
    "        for work_data in tqdm(works_query, total=works_query.count(), desc=\"Downloading\"):\n",
    "            try:\n",
    "                validated_work = CrossrefWorkModel.model_validate(work_data)\n",
    "                validated_records.append(validated_work)\n",
    "            except ValidationError as e:\n",
    "                failed_records.append({'doi': work_data.get('DOI'), 'error': str(e)})\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nDownload complete.\")\n",
    "    print(f\"Total validated records: {len(validated_records)}\")\n",
    "    if failed_records:\n",
    "        print(f\"Total records that failed validation: {len(failed_records)}\")\n",
    "\n",
    "#DEDUPE\n",
    "    unique_records = []\n",
    "    seen_dois = set()\n",
    "    duplicate_log = []\n",
    "\n",
    "    for work in validated_records:\n",
    "        doi = work.DOI\n",
    "\n",
    "        if doi not in seen_dois:\n",
    "            unique_records.append(work)\n",
    "            seen_dois.add(doi)\n",
    "        else:\n",
    "            duplicate_log.append({\n",
    "                \"DOI\": doi,\n",
    "                \"title\": work.title[0] if work.title else \"No Title\"\n",
    "            })\n",
    "\n",
    "\n",
    "    print(f\"Number of unique records after de-duplication: {len(unique_records)}\")\n",
    "    print(f\"Number of duplicate records found and removed: {len(duplicate_log)}\")\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([work.model_dump(by_alias=True) for work in unique_records])\n",
    "    print(f\"\\nCreated DataFrame with {len(df)} records for filtering.\")\n",
    "\n",
    "    if 'publisher' in df.columns:\n",
    "        # TODO:EXPAND LIST -- EXPANDED BELOW\n",
    "        academic_keywords = [\n",
    "            # --- Core Disciplines & Theories ---\n",
    "            'Philosophy', 'Philosophie', 'Filosofia', 'Filosofía',\n",
    "            'Literature', 'Literary', 'Linguistics', 'Poetics',\n",
    "            'Humanities', 'Theory', 'Critical', 'Deconstruction',\n",
    "            'Phenomenology', 'Psychoanalysis', 'Aesthetics', 'Cultural Studies',\n",
    "            \n",
    "            # --- Institutional & Publisher Types ---\n",
    "            'University Press', 'University', 'Press', 'Academic',\n",
    "            'College', 'Institute', 'Institut', 'Centro', 'Centre',\n",
    "            'Society', 'Société', 'Sociedad',\n",
    "            \n",
    "            # --- Publication Types (English) ---\n",
    "            'Journal', 'Review', 'Studies', 'Quarterly', 'Annual', 'Annals',\n",
    "            'Proceedings', 'Transactions', 'Bulletin', 'Archive', 'Yearbook',\n",
    "            \n",
    "            # --- Publication Types (Foreign Languages) ---\n",
    "            # French\n",
    "            'Revue', 'Cahiers', 'Études', 'Annales', 'Presses',\n",
    "            # German\n",
    "            'Zeitschrift', 'Kritik', 'Jahrbuch', 'Archiv', 'Verlag',\n",
    "            # Italian\n",
    "            'Rivista', 'Studi', 'Annali',\n",
    "            # Spanish / Portuguese\n",
    "            'Revista', 'Estudios', 'Anales',\n",
    "            # Latin\n",
    "            'Acta'\n",
    "        ]\n",
    "        \n",
    "        search_pattern = '|'.join(academic_keywords)\n",
    "        \n",
    "        df_filtered = df[df['publisher'].str.contains(search_pattern, case=False, na=False)]\n",
    "        return df_filtered.to_dict('records')\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nWarning: 'publisher' column not found in the downloaded data. Cannot perform filtering.\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blanchot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
